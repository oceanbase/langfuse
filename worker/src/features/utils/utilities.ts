import {
  ChatMessage,
  decryptAndParseExtraHeaders,
  fetchLLMCompletion,
  logger,
  type TraceParams,
} from "@langfuse/shared/src/server";
import { ApiError, LLMApiKeySchema, ZodModelConfig } from "@langfuse/shared";
import { z } from "zod/v4";
import { z as zodV3 } from "zod/v3";
import { ZodSchema as ZodV3Schema } from "zod/v3";
import { decrypt } from "@langfuse/shared/encryption";
import { tokenCount } from "../tokenisation/usage";
import Handlebars from "handlebars";

/**
 * Standard error handling for LLM operations
 * Handles common LLM errors like quota limits and throttling with appropriate status codes
 *
 * @param operation - The async LLM operation to execute
 * @param operationName - Name for error context (e.g., "call LLM")
 * @returns The result of the operation or throws an ApiError
 */
async function withLLMErrorHandling<T>(
  operation: () => Promise<T>,
  operationName: string = "LLM operation",
): Promise<T> {
  try {
    return await operation();
  } catch (e) {
    // Handle specific LLM provider errors with appropriate status codes
    if (
      e instanceof Error &&
      (e.name === "InsufficientQuotaError" || e.name === "ThrottlingException")
    ) {
      throw new ApiError(e.name, 429);
    }

    // Handle all other errors with preserved status codes
    throw new ApiError(
      `Failed to ${operationName}: ${e}`,
      (e as any)?.response?.status ?? (e as any)?.status,
    );
  }
}

export async function callStructuredLLM<T extends ZodV3Schema>(
  jeId: string,
  llmApiKey: z.infer<typeof LLMApiKeySchema>,
  messages: ChatMessage[],
  modelParams: z.infer<typeof ZodModelConfig>,
  provider: string,
  model: string,
  structuredOutputSchema: T,
): Promise<zodV3.infer<T>> {
  logger.info(`ğŸš€ å¼€å§‹ç»“æ„åŒ– LLM è°ƒç”¨ - ä»»åŠ¡ID: ${jeId}`, {
    jobExecutionId: jeId,
    provider,
    model,
    baseURL: llmApiKey.baseURL,
    adapter: llmApiKey.adapter,
    messagesCount: messages.length,
    hasExtraHeaders: !!llmApiKey.extraHeaders,
  });

  const startTime = Date.now();
  let result;
  let success = false;
  let error = null;

  try {
    result = await withLLMErrorHandling(async () => {
      logger.info(`ğŸ“¡ å‘é€è¯·æ±‚åˆ° LLM - ä»»åŠ¡ID: ${jeId}, æ¨¡å‹: ${model}`, {
        jobExecutionId: jeId,
        model,
        provider,
        baseURL: llmApiKey.baseURL,
      });

      const { completion } = await fetchLLMCompletion({
        streaming: false,
        apiKey: decrypt(llmApiKey.secretKey), // decrypt the secret key
        extraHeaders: decryptAndParseExtraHeaders(llmApiKey.extraHeaders),
        baseURL: llmApiKey.baseURL || undefined,
        messages,
        modelParams: {
          provider,
          model,
          adapter: llmApiKey.adapter,
          ...modelParams,
        },
        structuredOutputSchema,
        config: llmApiKey.config,
        maxRetries: 1,
      });

      logger.info(`ğŸ“¥ æ”¶åˆ° LLM å“åº” - ä»»åŠ¡ID: ${jeId}`, {
        jobExecutionId: jeId,
        model,
        responseType: typeof completion,
        hasCompletion: !!completion,
      });

      // ä¸ºåƒé—®æ¨¡å‹æ·»åŠ è¯¦ç»†çš„å“åº”æ—¥å¿—å’Œå­—æ®µæ˜ å°„å¤„ç†
      if (
        model.toLowerCase().includes("qwen") &&
        (provider.toLowerCase().includes("qianwen") ||
          provider.toLowerCase().includes("qwen"))
      ) {
        logger.info(`ğŸ” åƒé—®æ¨¡å‹åŸå§‹å“åº”è¯¦æƒ… - ä»»åŠ¡ID: ${jeId}`, {
          jobExecutionId: jeId,
          model,
          provider,
          completionType: typeof completion,
          completionKeys:
            completion && typeof completion === "object"
              ? Object.keys(completion)
              : "N/A",
          completionPreview: completion
            ? JSON.stringify(completion, null, 2).substring(0, 1000)
            : "N/A",
          schemaType: structuredOutputSchema
            ? structuredOutputSchema.constructor.name
            : "N/A",
        });

        // ä¸ºåƒé—®æ¨¡å‹æ·»åŠ å­—æ®µåç§°æ˜ å°„å¤„ç†
        if (
          completion &&
          typeof completion === "object" &&
          completion !== null
        ) {
          const mappedCompletion = { ...(completion as Record<string, any>) };

          // å¤„ç†å¯èƒ½çš„å­—æ®µåç§°ä¸åŒ¹é…
          if (mappedCompletion.reason && !mappedCompletion.reasoning) {
            mappedCompletion.reasoning = mappedCompletion.reason;
            delete mappedCompletion.reason;
            logger.info(
              `ğŸ”§ åƒé—®æ¨¡å‹å­—æ®µæ˜ å°„: reason -> reasoning - ä»»åŠ¡ID: ${jeId}`,
              {
                jobExecutionId: jeId,
                model,
                provider,
              },
            );
          }

          if (mappedCompletion.rating && !mappedCompletion.score) {
            mappedCompletion.score = mappedCompletion.rating;
            delete mappedCompletion.rating;
            logger.info(
              `ğŸ”§ åƒé—®æ¨¡å‹å­—æ®µæ˜ å°„: rating -> score - ä»»åŠ¡ID: ${jeId}`,
              {
                jobExecutionId: jeId,
                model,
                provider,
              },
            );
          }

          // å¦‚æœå­—æ®µä»ç„¶ç¼ºå¤±ï¼Œå°è¯•ä»å…¶ä»–å¯èƒ½çš„å­—æ®µä¸­è·å–
          if (!mappedCompletion.reasoning && mappedCompletion.explanation) {
            mappedCompletion.reasoning = mappedCompletion.explanation;
            delete mappedCompletion.explanation;
            logger.info(
              `ğŸ”§ åƒé—®æ¨¡å‹å­—æ®µæ˜ å°„: explanation -> reasoning - ä»»åŠ¡ID: ${jeId}`,
              {
                jobExecutionId: jeId,
                model,
                provider,
              },
            );
          }

          if (!mappedCompletion.score && mappedCompletion.value) {
            mappedCompletion.score = mappedCompletion.value;
            delete mappedCompletion.value;
            logger.info(
              `ğŸ”§ åƒé—®æ¨¡å‹å­—æ®µæ˜ å°„: value -> score - ä»»åŠ¡ID: ${jeId}`,
              {
                jobExecutionId: jeId,
                model,
                provider,
              },
            );
          }

          // è®°å½•æ˜ å°„åçš„ç»“æœ
          logger.info(`ğŸ”§ åƒé—®æ¨¡å‹å­—æ®µæ˜ å°„åç»“æœ - ä»»åŠ¡ID: ${jeId}`, {
            jobExecutionId: jeId,
            model,
            provider,
            mappedKeys: Object.keys(mappedCompletion),
            mappedPreview: JSON.stringify(mappedCompletion, null, 2).substring(
              0,
              1000,
            ),
          });

          return structuredOutputSchema.parse(mappedCompletion);
        }
      }

      return structuredOutputSchema.parse(completion);
    }, "call LLM");

    const endTime = Date.now();
    const duration = endTime - startTime;
    success = true;

    logger.info(
      `âœ… ç»“æ„åŒ– LLM è°ƒç”¨æˆåŠŸ - ä»»åŠ¡ID: ${jeId}, è€—æ—¶: ${duration}ms`,
      {
        jobExecutionId: jeId,
        duration,
        model,
        provider,
        result: result,
      },
    );

    return result;
  } catch (err) {
    const endTime = Date.now();
    const duration = endTime - startTime;
    success = false;
    error = err;

    logger.error(
      `âŒ ç»“æ„åŒ– LLM è°ƒç”¨å¤±è´¥ - ä»»åŠ¡ID: ${jeId}, è€—æ—¶: ${duration}ms`,
      {
        jobExecutionId: jeId,
        duration,
        model,
        provider,
        error: err instanceof Error ? err.message : String(err),
        errorStack: err instanceof Error ? err.stack : undefined,
      },
    );

    throw err;
  }
}

export async function callLLM(
  llmApiKey: z.infer<typeof LLMApiKeySchema>,
  messages: ChatMessage[],
  modelParams: z.infer<typeof ZodModelConfig>,
  provider: string,
  model: string,
  traceParams?: Omit<TraceParams, "tokenCountDelegate">,
): Promise<string> {
  return withLLMErrorHandling(async () => {
    const { completion, processTracedEvents } = await fetchLLMCompletion({
      streaming: false,
      apiKey: decrypt(llmApiKey.secretKey),
      extraHeaders: decryptAndParseExtraHeaders(llmApiKey.extraHeaders),
      baseURL: llmApiKey.baseURL || undefined,
      messages,
      modelParams: {
        provider,
        model,
        adapter: llmApiKey.adapter,
        ...modelParams,
      },
      config: llmApiKey.config,
      traceParams: traceParams
        ? { ...traceParams, tokenCountDelegate: tokenCount }
        : undefined,
      maxRetries: 1,
      throwOnError: false,
    });

    if (traceParams) {
      await processTracedEvents();
    }

    return completion;
  }, "call LLM");
}

export function compileHandlebarString(
  handlebarString: string,
  context: Record<string, any>,
): string {
  try {
    const template = Handlebars.compile(handlebarString, { noEscape: true });
    return template(context);
  } catch (error) {
    logger.info("Handlebars compilation error:", error);
    return handlebarString; // Fallback to the original string if Handlebars fails
  }
}
