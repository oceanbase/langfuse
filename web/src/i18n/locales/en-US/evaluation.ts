const translation = {
  eval: {
    tabs: {
      runningEvaluators: "Running Evaluators",
      evaluatorLibrary: "Evaluator Library",
    },
    form: {
      scoreRangePrompt: "Score range prompt",
      scoreRangePromptDescription: "Define how the LLM should return the evaluation score in natural language. Needs to yield a numeric value.",
      selectTemplateName: "Select a template name",
      model: "Model",
      useDefaultEvaluationModel: "Use default evaluation model",
      noDefaultModelSet: "No default model set. Set up default evaluation model",
      customModelConfiguration: "Custom model configuration",
      selectModelWithFunctionCalling: "Select a model which supports function calling.",
      prompt: "Prompt",
      evaluationPrompt: "Evaluation prompt",
      defineLlmAsJudgeTemplate: 'Define your llm-as-a-judge evaluation template. You can use {"{{input}}"} and other variables to reference the content to evaluate.',
      scoreReasoningPrompt: "Score reasoning prompt",
      defineLlmExplanation: "Define how the LLM should explain its evaluation. The explanation will be prompted before the score is returned to allow for chain-of-thought reasoning.",
      showPreview: "Show Preview",
      generatedScoreName: "Generated Score Name",
      target: "Target",
      targetData: "Target data",
      evaluatorTargetDataTooltip: "An evaluator's target data may only be configured at creation.",
      liveTracingData: "Live tracing data",
      datasetRuns: "Dataset runs",
      evaluatorRunsOn: "Evaluator runs on",
      new: "New",
      existing: "Existing",
      traces: "traces",
      datasetRunItems: "dataset run items",
      cannotChangeExistingTraces: "The evaluator ran on existing traces already. This cannot be changed anymore.",
      pleaseSelectAtLeastOne: "Please select at least one.",
      pleaseFillOutAllFilterFields: "Please fill out all filter fields",
      pleaseFillOutAllVariableMappings: "Please fill out all variable mappings",
      previewSampleMatchedTraces: "Preview sample matched traces",
      sampleOverLast24Hours: "Sample over the last 24 hours that match these filters",
      evaluationPromptPreview: "Evaluation Prompt Preview",
      noTraceDataFound: "No trace data found, please adjust filters or switch to not show preview.",
      variableMapping: "Variable mapping",
      previewEvaluationPrompt: "Preview of the evaluation prompt with the variables replaced with the first matched trace data subject to the filters.",
      object: "Object",
      objectName: "Object Name",
      objectVariable: "Object Variable",
      jsonPath: "JsonPath",
      optionalSelection: "Optional selection: Use JsonPath syntax to select from a JSON object stored on a trace. If not selected, we will pass the entire object into the prompt.",
      optional: "Optional",
      enterName: "Enter name...",
      oneSentenceReasoning: "One sentence reasoning for the score",
      enterLangfuseObjectName: "Enter Langfuse object name",
      variableInTemplate: "Variable in the template to be replaced with the mapped data.",
      langfuseObjectToRetrieve: "Langfuse object to retrieve the data from.",
      nameOfLangfuseObject: "Name of the Langfuse object to retrieve the data from.",
      variableOnLangfuseObject: "Variable on the Langfuse object to insert into the template.",
      targetFilter: "Target filter",
      sampling: "Sampling",
      delaySeconds: "Delay (seconds)",
      timeBetweenFirstTrace: "Time between first Trace/Dataset run event and evaluation execution to ensure all data is available",
      thisEvaluatorHasAlreadyRun: "This evaluator has already run on existing {{type}} once. Set up a new evaluator to re-run on existing {{type}}.",
      selectTimeScope: "Select a time scope to run this configuration on.",
    },
    buttons: {
      createCustomEvaluator: "Create Custom Evaluator",
      useSelectedEvaluator: "Use Selected Evaluator",
    },
    messages: {
      noEvaluatorsFound: "No evaluators found. Create a new evaluator to get started.",
      updatedEvaluators: "Updated evaluators",
      updatedReferencedEvaluators: "Updated referenced evaluators to use new template version.",
    },
    errors: {
      selectProvider: "Select a provider",
      selectModel: "Select a model",
      enterName: "Enter a name",
      enterPrompt: "Enter a prompt",
      variablesMustContainLetters: "Variables must only contain letters and underscores (_)",
      variablesMinLength: "Variables must have at least one character",
      enterScoreFunction: "Enter a score function",
      enterReasoningFunction: "Enter a reasoning function",
      noDefaultEvaluationModelSet: "No default evaluation model set. Set up default evaluation model or use a custom model",
      evaluatorConfiguredButNoApiKey: "This evaluator is configured to use {{provider}}s models but no API key exists. Add a key or choose another provider.",
      pleaseTypeCorrectConfirmation: "Please type the correct confirmation",
    },
    labels: {
      evaluationPrompt: "Evaluation Prompt",
      object: "Object",
      objectDescription: "Langfuse object to retrieve the data from.",
      objectName: "Object Name",
      objectNameDescription: "Name of the Langfuse object to retrieve the data from.",
      objectVariable: "Object Variable",
      objectVariableDescription: "Variable on the Langfuse object to insert into the template.",
      jsonPath: "JsonPath",
      jsonPathDescription: "Optional selection: Use JsonPath syntax to select from a JSON object stored on a trace. If not selected, we will pass the entire object into the prompt.",
      objectType: "Object type",
      enterLangfuseObjectName: "Enter Langfuse object name",
      optional: "Optional",
    },
    success: {
      updatedEvaluators: "Updated evaluators",
      updatedReferencedEvaluators: "Updated referenced evaluators to use new template version.",
      evaluatorCreatedSuccessfully: "Evaluator created successfully",
      youCanNowUseThisEvaluator: "You can now use this evaluator.",
      defaultEvaluationModelUpdated: "Default evaluation model updated",
      allRunningEvaluatorsWillUseNewModel: "All running evaluators will use the new model.",
    },
    dialog: {
      createNewEvaluator: "Create new evaluator",
    },
    pages: {
      defaultEvaluationModel: "Default Evaluation Model",
      configureDefaultEvaluationModel: "Configure a default evaluation model for your project.",
      evaluatorLibrary: "Evaluator Library",
      defaultModel: "Default model",
      noDefaultModelSet: "No default model set. Set up default evaluation model",
      defaultModelConfiguration: "Default model configuration",
      selectModelWithFunctionCalling: "Select a model which supports function calling.",
      updatingDefaultModelWillImpact: "Updating the default model will impact any currently running evaluators that use it. Please confirm that you want to proceed with this change.",
      typeToConfirm: 'Type "{{confirmation}}" to confirm',
      runningEvaluator: "Running evaluator",
      editMode: "Edit Mode",
      configuration: "Configuration",
      active: "active",
      referencedEvaluator: "Referenced Evaluator",
      where: "Where",
      dataset: "Dataset",
      noneOf: "none of",
      select: "Select",
      itemsSelected: "{{count}} items selected",
      thisConfigurationWillTarget: "This configuration will target {{scope}} {{target}} that match these filters.",
      allFutureAndExisting: "all future and existing",
      allFuture: "all future",
      allExisting: "all existing",
      traces: "traces",
      datasetRunItems: "dataset run items",
      langfuseMaintained: "Langfuse maintained",
      ragasMaintained: "Ragas maintained",
      userMaintained: "User maintained",
      notAvailable: "Not available",
      scoreBetweenZeroAndOne: "Score between 0 and 1. Score 0 if false or negative and 1 if true or positive.",
      evaluatorSimpleCriteria: "Evaluator Simple Criteria",
      evaluateInputBasedOnCriteria: "Evaluate the input based on the criteria defined.",
      criteriaDefinition: "Criteria Definition: {{criteria_definition}}",
      input: "Input: {{input}}",
      theFollowingVariablesAreAvailable: "The following variables are available:",
      oneSentenceReasoningForScore: "One sentence reasoning for the score",
      llmAsJudgeEvaluators: "LLM-as-a-Judge Evaluators",
      getStartedTitle: "Get Started with LLM-as-a-Judge Evaluations",
      getStartedDescription:
        "Create evaluation templates and evaluators to automatically score your traces with LLM-as-a-judge. Set up custom evaluation criteria and let AI help you measure the quality of your outputs.",
      createEvaluator: "Create Evaluator",
      learnMore: "Learn More",
      configureEvaluatorDescription: "Configure a Langfuse managed or custom evaluator to evaluate incoming traces.",
      viewAllEvaluatorsDescription: "View all Langfuse managed and custom evaluators.",
      valuePropositions: {
        automateEvaluations: {
          title: "Automate evaluations",
          description: "Use LLM-as-a-judge to automatically evaluate your traces without manual review",
        },
        measureQuality: {
          title: "Measure quality",
          description: "Create custom evaluation criteria to measure the quality of your LLM outputs",
        },
        scaleEfficiently: {
          title: "Scale efficiently",
          description: "Evaluate thousands of traces automatically with customizable sampling rates",
        },
        trackPerformance: {
          title: "Track performance",
          description: "Monitor evaluation metrics over time to identify trends and improvements",
        },
      },
      evalTemplates: "Eval Templates",
      newTemplate: "New template",
      evaluators: "Evaluators",
      templates: "Templates",
      log: "Log",
      name: "Name",
      lastEdit: "Last Edit",
      lastVersion: "Last Version",
      rowsPerPage: "Rows per page",
      page: "Page",
      of: "of",
      selectEvaluators: "Select evaluators",
      searchEvaluators: "Search evaluators...",
    },
    actions: {
      setUp: "Set up evaluator",
      runningEvaluatorDeleted: "Running evaluator deleted",
      runningEvaluatorDeletedDescription: "The running evaluator has been deleted successfully",
      defaultEvaluationModelDeleted: "Default evaluation model deleted",
      defaultEvaluationModelDeletedDescription: "The default evaluation model has been deleted. Any running evaluations relying on the default model will be inactivated. Queued jobs will fail.",
    },
    confirmations: {
      runningEvaluatorDeletePrompt: "This action cannot be undone and removes all logs associated with this running evaluator. Scores produced by this evaluator will not be deleted.",
      defaultModelDeletePrompt: "Deleting this model might cause running evaluators to fail. Please make sure you have no running evaluators relying on this model.",
    },
    templateDetail: {
      evaluatorLibrary: "Evaluator Library",
      changeHistory: "Change history",
      viewOnly: "View only",
      editMode: "Edit mode",
    },
    templatesTable: {
      maintainer: "Maintainer",
      lastEdit: "Last Edit",
      usageCount: "Usage count",
      latestVersion: "Latest Version",
      useEvaluator: "Use Evaluator",
      evaluatorRequiresProjectLevel: "Evaluator requires project-level evaluation model. Set it up and start running evaluations.",
      evaluatorClonedSuccessfully: "Evaluator cloned successfully",
      evaluatorClonedDescription: "This evaluator is now available and maintained on project level.",
      errorCloningEvaluator: "Error cloning evaluator",
      editEvaluator: "Edit evaluator",
      evaluatorUpdatedSuccessfully: "Evaluator updated successfully",
      evaluatorUpdatedDescription: "You can now use this evaluator.",
      cloneEvaluator: "Clone evaluator",
      updateRunningEvaluators: "Update running evaluators?",
      updateRunningEvaluatorsDescription: "Do you want all running evaluators attached to the original Langfuse evaluator to reference your new project-level version?",
      warning: "Warning:",
      warningDescription: "This might break workflows if you have changed variables or other critical aspects of the template.",
      noKeepAsIs: "No, keep as is",
      yesUpdateAllReferences: "Yes, update all references",
    },
    evaluatorTable: {
      generatedScoreName: "Generated Score Name",
      result: "Result",
      logs: "Logs",
      referencedEvaluator: "Referenced Evaluator",
      target: "Target",
      filter: "Filter",
      templateNotFound: "template not found",
      editConfiguration: "Edit configuration",
      evaluatorUpdatedSuccessfully: "Evaluator updated successfully",
      changesReflectedFutureRuns: "Changes will automatically be reflected future evaluator runs",
      userMaintained: "User maintained",
      langfuseMaintained: "Langfuse maintained",
      langfuseAndRagasMaintained: "Langfuse and Ragas maintained",
      notAvailable: "Not available",
    },
    logTable: {
      startTime: "Start Time",
      endTime: "End Time",
      scoreName: "Score Name",
      scoreValue: "Score Value",
      scoreComment: "Score Comment",
      trace: "Trace",
      template: "Template",
      evaluator: "Evaluator",
    },
    newEvaluator: {
      title: "Set up evaluator",
      runningEvaluators: "Running Evaluators",
      selectEvaluator: "1. Select Evaluator",
      runEvaluator: "2. Run Evaluator",
      noAccess: "You do not have access to this page.",
      noDefaultModelSet: "No default model set",
      searchEvaluators: "Search evaluators...",
      langfuseManagedEvaluators: "Langfuse managed evaluators",
      noEvaluatorFound: "No evaluator found.",
      requiresProjectLevelEvaluationModel: "Requires project-level evaluation model",
      selectEvaluators: "Select evaluators",
      activeEvaluators: "{{count}} active evaluators",
    },
  },
  score: {
    pages: {
      title: "Scores",
      description: "A scores is an evaluation of a traces or observations. It can be created from user feedback, model-based evaluations, or manual review. See docs to learn more.",
    },
    onboarding: {
      getStartedTitle: "Get Started with Scores",
      getStartedDescription:
        "Scores allow you to evaluate the quality/safety of your LLM application through user feedback, model-based evaluations, or manual review. Scores can be used programmatically via the API and SDKs to track custom metrics.",
      learnMore: "Learn More",
    },
    features: {
      collectUserFeedback: {
        title: "Collect user feedback",
        description: "Gather thumbs up/down feedback from users to identify high and low quality outputs",
      },
      runModelBasedEvaluations: {
        title: "Run model-based evaluations",
        description: "Use LLMs to automatically evaluate your application's outputs",
      },
      trackQualityMetrics: {
        title: "Track quality metrics",
        description: "Monitor quality metrics over time to identify trends and issues",
      },
      useCustomMetrics: {
        title: "Use custom metrics",
        description: "Langfuse's scores are flexible and can be used to track any metric that's associated with an LLM application",
      },
    },
    dashboard: {
      title: "Dashboard",
      modelLatencies: "Model latencies",
      latenciesPerGeneration: "Latencies (seconds) per LLM generation",
      percentiles: {
        p50: "50th Percentile",
        p75: "75th Percentile",
        p90: "90th Percentile",
        p95: "95th Percentile",
        p99: "99th Percentile",
      },
      scoresAnalytics: "Scores Analytics",
      aggregateScoresAndAverages: "Aggregate scores and averages over time",
      selectScoreToViewAnalytics: "Select a score to view analytics",
      searchScore: "Search score...",
      select: "Select",
    },
    errors: {
      maxValueMustBeGreater: "Maximum value must be greater than Minimum value.",
      atLeastOneCategoryRequired: "At least one category is required for categorical data types.",
      booleanMustHaveTwoCategories: "Boolean data type must have exactly 2 categories.",
      booleanMustHaveZeroAndOne: "Boolean data type must have categories with values 0 and 1.",
      categoryNamesMustBeUnique: "Category names must be unique.",
      categoryValuesMustBeUnique: "Category values must be unique.",
      errorCreatingConfig: "An error occurred while creating config.",
    },
    form: {
      dataType: "Data type",
      selectDataType: "Select a data type",
      minimumOptional: "Minimum (optional)",
      maximumOptional: "Maximum (optional)",
      value: "Value",
      label: "Label",
      addNewScoreConfig: "Add new score config",
      addNewScoreConfigTitle: "Add new score config",
      addCategory: "Add category",
      removeCategory: "Remove category",
      description: "Description",
      descriptionOptional: "Description (optional)",
      provideOptionalDescription: "Provide an optional description of the score config...",
    },
    confirmDialog: {
      title: "Score configs cannot be edited or deleted after they have been created. Are you sure you want to proceed?",
      continueEditing: "Continue Editing",
    },
    configSettings: {
      title: "Score Configs",
      description: "Score configs define which scores are available for annotation in your project. Please note that all score configs are immutable.",
      annotation: "annotation",
    },
  },
};

export default translation;
